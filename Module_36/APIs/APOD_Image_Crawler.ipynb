{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APOD Image Crawler\n",
    "This is a web crawler developed with scrapy. It queries all APOD image explanations and dates for all the dates in a specific year. The output is saved in JSON format for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "from config import api_key\n",
    "\n",
    "year_dict = {\n",
    "    1: 31,\n",
    "    2: 28,\n",
    "    3: 31,\n",
    "    4: 30,\n",
    "    5: 31,\n",
    "    6: 30,\n",
    "    7: 31,\n",
    "    8: 31,\n",
    "    9: 30,\n",
    "    10: 31,\n",
    "    11: 30,\n",
    "    12: 31\n",
    "}\n",
    "\n",
    "year = '2018'\n",
    "date_string = ''\n",
    "url_string = ''\n",
    "    \n",
    "class APODSpider(scrapy.Spider):\n",
    "    name = \"APOD\"\n",
    "    \n",
    "    start_urls = []\n",
    "    \n",
    "    # Loop through each day of the year\n",
    "    for month, days in year_dict.items():\n",
    "        m_string = f'{month}'\n",
    "        if len(m_string) == 1:\n",
    "            m_string = f'0{m_string}'\n",
    "        for d in range(1, days+1):\n",
    "            d_string = f'{d}'\n",
    "            if len(d_string) == 1:\n",
    "                d_string = f'0{d_string}'\n",
    "            date_string = f'{year}-{m_string}-{d_string}'\n",
    "            \n",
    "            # API call to APOD.\n",
    "            url_string = f'https://api.nasa.gov/planetary/apod?api_key={api_key}&date={date_string}'\n",
    "            \n",
    "            # Append URLs list list\n",
    "            start_urls.append(url_string)\n",
    "        \n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        for item in response.xpath('//p/text()'):\n",
    "            yield {\n",
    "                'date': json.loads(item.get())['date'],\n",
    "                'explanation': json.loads(item.get())['explanation'],\n",
    "                'media_type': json.loads(item.get())['media_type']\n",
    "#                 'text': json.loads(item.get())\n",
    "            }\n",
    "        \n",
    "# Configure crawler\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': './data/APOD.json',\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'DavidClarkAPICrawler [dmclark5@gmail.com]',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,   \n",
    "    'CLOSESPIDER_PAGECOUNT' : 365\n",
    "})\n",
    "    \n",
    "# Start crawler.\n",
    "process.crawl(APODSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
